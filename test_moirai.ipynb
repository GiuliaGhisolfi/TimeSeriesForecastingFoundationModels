{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5659025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.evaluation.save_results import create_output_file, save_results\n",
    "\n",
    "output_file_name = \"moirai_small_test_results.csv\"\n",
    "\n",
    "output_dir=\"results/test\"\n",
    "csv_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "create_output_file(output_file_name, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ffa6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from src.evaluation.load_chronos_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4429873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.metrics import get_metrics\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f5d5e",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ab7e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "short_datasets = \"solar/10T solar/H solar/D solar/W jena_weather/10T jena_weather/H jena_weather/D \" \\\n",
    "\"bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application \" \\\n",
    "\"bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "\n",
    "med_long_datasets = \"solar/10T solar/H jena_weather/10T jena_weather/H \" \\\n",
    "\"bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"./data/dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee355eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valerio/miniconda3/envs/moirai/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/valerio/miniconda3/envs/moirai/lib/python3.9/site-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 175 files: 100%|██████████| 175/175 [00:00<00:00, 1394.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets in GIFT_EVAL:\n",
      "- LOOP_SEATTLE/5T\n",
      "- LOOP_SEATTLE/D\n",
      "- LOOP_SEATTLE/H\n",
      "- M_DENSE/D\n",
      "- M_DENSE/H\n",
      "- SZ_TAXI/15T\n",
      "- SZ_TAXI/H\n",
      "- bitbrains_fast_storage/5T\n",
      "- bitbrains_fast_storage/H\n",
      "- bitbrains_rnd/5T\n",
      "- bitbrains_rnd/H\n",
      "- bizitobs_application\n",
      "- bizitobs_l2c/5T\n",
      "- bizitobs_l2c/H\n",
      "- bizitobs_service\n",
      "- car_parts_with_missing\n",
      "- covid_deaths\n",
      "- electricity/15T\n",
      "- electricity/D\n",
      "- electricity/H\n",
      "- electricity/W\n",
      "- ett1/15T\n",
      "- ett1/D\n",
      "- ett1/H\n",
      "- ett1/W\n",
      "- ett2/15T\n",
      "- ett2/D\n",
      "- ett2/H\n",
      "- ett2/W\n",
      "- hierarchical_sales/D\n",
      "- hierarchical_sales/W\n",
      "- hospital\n",
      "- jena_weather/10T\n",
      "- jena_weather/D\n",
      "- jena_weather/H\n",
      "- kdd_cup_2018_with_missing/D\n",
      "- kdd_cup_2018_with_missing/H\n",
      "- m4_daily\n",
      "- m4_hourly\n",
      "- m4_monthly\n",
      "- m4_quarterly\n",
      "- m4_weekly\n",
      "- m4_yearly\n",
      "- restaurant\n",
      "- saugeenday/D\n",
      "- saugeenday/M\n",
      "- saugeenday/W\n",
      "- solar/10T\n",
      "- solar/D\n",
      "- solar/H\n",
      "- solar/W\n",
      "- temperature_rain_with_missing\n",
      "- us_births/D\n",
      "- us_births/M\n",
      "- us_births/W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.evaluation.load_data import load_gift_data\n",
    "\n",
    "load_gift_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a2a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = [\n",
    "    'bizitobs_l2c/5T',\n",
    "    'bitbrains_fast_storage/5T',\n",
    "    'bitbrains_rnd/H',\n",
    "    'bizitobs_l2c/H',\n",
    "    'jena_weather/H',\n",
    "    'bizitobs_application',\n",
    "    'bizitobs_service',\n",
    "    'bitbrains_fast_storage/H',\n",
    "    'solar/D',\n",
    "    'jena_weather/10T',\n",
    "    'solar/W',\n",
    "    'jena_weather/D',\n",
    "    'bitbrains_rnd/5T',\n",
    "    'solar/10T',\n",
    "    'solar/H',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f29123ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHRONOS_DATASET_NAME = [\"exchange_rate\", \"ercot\", \"dominick\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b296a39",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3271f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"moirai_checkpoints/\"\n",
    "MODEL_NAME = \"moirai_small\"\n",
    "MODULE = \"Salesforce/moirai-1.1-R-small\"\n",
    "\n",
    "CHECKPOINTS = [f\"moirai_small_epoch_epoch={i}.ckpt\" for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "746a02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ff478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: exchange_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "from src.evaluation.moirai_predictor import load_predictor\n",
    "from src.gift_eval.data import Dataset\n",
    "\n",
    "for train_step, model_name in enumerate(CHECKPOINTS):    \n",
    "    for ds_name in CHRONOS_DATASET_NAME:\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            ds_config = f\"{ds_name}/{term}\"\n",
    "\n",
    "            dataset, prediction_length, frequency, domain, num_variates = load_data(ds_name, term)\n",
    "            season_length = get_seasonality(frequency)\n",
    "\n",
    "            predictor = load_predictor(\n",
    "                checkpoint=MODEL_PATH+model_name, \n",
    "                module=MODULE,\n",
    "                prediction_length=prediction_length, \n",
    "                target_dim=num_variates,\n",
    "                device_map=\"cpu\"\n",
    "                )\n",
    "\n",
    "            res = evaluate_model(\n",
    "                predictor,\n",
    "                test_data=dataset,\n",
    "                metrics=metrics,\n",
    "                batch_size=1024,\n",
    "                axis=None,\n",
    "                mask_invalid_label=True,\n",
    "                allow_nan_forecast=False,\n",
    "                seasonality=season_length,\n",
    "            )\n",
    "\n",
    "            # Append the results to the CSV file\n",
    "            save_results(res, ds_config, MODEL_NAME, train_step, domain, num_variates, ds_name, csv_file_path)\n",
    "\n",
    "    for ds_name in DATASET_NAME:\n",
    "        ds_key = ds_name.split(\"/\")[0]\n",
    "        print(f\"Processing dataset: {ds_name}\")\n",
    "        terms = [\"short\", \"medium\", \"long\"]\n",
    "        for term in terms:\n",
    "            if (\n",
    "                term == \"medium\" or term == \"long\"\n",
    "            ) and ds_name not in med_long_datasets.split():\n",
    "                continue\n",
    "\n",
    "            if \"/\" in ds_name:\n",
    "                ds_key = ds_name.split(\"/\")[0]\n",
    "                ds_freq = ds_name.split(\"/\")[1]\n",
    "                ds_key = ds_key.lower()\n",
    "            else:\n",
    "                ds_key = ds_name.lower()\n",
    "                ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "            ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "            # Initialize the dataset\n",
    "            to_univariate = (\n",
    "                False\n",
    "                if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "                else True\n",
    "            )\n",
    "            dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "            season_length = get_seasonality(dataset.freq)\n",
    "\n",
    "            predictor = load_predictor(\n",
    "                checkpoint=MODEL_PATH+model_name, \n",
    "                module=MODULE,\n",
    "                prediction_length=prediction_length, \n",
    "                target_dim=num_variates,\n",
    "                device_map=\"cpu\"\n",
    "                )\n",
    "\n",
    "            # Measure the time taken for evaluation\n",
    "            res = evaluate_model(\n",
    "                predictor,\n",
    "                test_data=dataset.test_data,\n",
    "                metrics=metrics,\n",
    "                batch_size=1024,\n",
    "                axis=None,\n",
    "                mask_invalid_label=True,\n",
    "                allow_nan_forecast=False,\n",
    "                seasonality=season_length,\n",
    "            )\n",
    "\n",
    "            # Append the results to the CSV file\n",
    "            save_results(res, ds_config, MODEL_NAME, train_step, domain, num_variates, ds_name, csv_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moirai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
